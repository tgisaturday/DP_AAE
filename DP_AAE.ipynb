{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tgisaturday/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting ./MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ./MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import os\n",
    "\n",
    "mb_size = 32\n",
    "X_dim = 784\n",
    "z_dim = 10\n",
    "h_dim = 128\n",
    "\n",
    "num_hidden_1 = 256 # 1st layer num features\n",
    "num_hidden_2 = 128 # 2nd layer num features (the latent dim)\n",
    "num_input = 784 # MNIST data input (img shape: 28*28)\n",
    "\n",
    "mnist = input_data.read_data_sets('./MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(samples):\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "    gs = gridspec.GridSpec(8, 8)\n",
    "    gs.update(wspace=0.05, hspace=0.05)\n",
    "        \n",
    "    for i, sample in enumerate(samples):\n",
    "        ax = plt.subplot(gs[i])\n",
    "        plt.axis('off')\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_aspect('equal')\n",
    "        plt.imshow(sample.reshape(28, 28), cmap='Greys_r')\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def xavier_init(size):\n",
    "    in_dim = size[0]\n",
    "    xavier_stddev = 1. / tf.sqrt(in_dim / 2.)\n",
    "    return tf.random_normal(shape=size, stddev=xavier_stddev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[None, X_dim])\n",
    "\n",
    "D_W1 = tf.Variable(xavier_init([X_dim, h_dim]))\n",
    "D_b1 = tf.Variable(tf.zeros(shape=[h_dim]))\n",
    "\n",
    "D_W2 = tf.Variable(xavier_init([h_dim, 1]))\n",
    "D_b2 = tf.Variable(tf.zeros(shape=[1]))\n",
    "\n",
    "theta_D = [D_W1, D_W2, D_b1, D_b2]\n",
    "\n",
    "weights = {\n",
    "    'encoder_h1': tf.Variable(tf.random_normal([num_input, num_hidden_1])),\n",
    "    'encoder_h2': tf.Variable(tf.random_normal([num_hidden_1, num_hidden_2])),\n",
    "    'decoder_h1': tf.Variable(tf.random_normal([num_hidden_2, num_hidden_1])),\n",
    "    'decoder_h2': tf.Variable(tf.random_normal([num_hidden_1, num_input])),\n",
    "}\n",
    "biases = {\n",
    "    'encoder_b1': tf.Variable(tf.random_normal([num_hidden_1])),\n",
    "    'encoder_b2': tf.Variable(tf.random_normal([num_hidden_2])),\n",
    "    'decoder_b1': tf.Variable(tf.random_normal([num_hidden_1])),\n",
    "    'decoder_b2': tf.Variable(tf.random_normal([num_input])),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(x):\n",
    "    # Encoder Hidden layer with sigmoid activation #1\n",
    "    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['encoder_h1']),\n",
    "                                   biases['encoder_b1']))\n",
    "    # Encoder Hidden layer with sigmoid activation #2\n",
    "    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['encoder_h2']),\n",
    "                                   biases['encoder_b2']))\n",
    "    return layer_2\n",
    "\n",
    "def decoder(x):\n",
    "    # Decoder Hidden layer with sigmoid activation #1\n",
    "    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['decoder_h1']),\n",
    "                                   biases['decoder_b1']))\n",
    "    # Decoder Hidden layer with sigmoid activation #2\n",
    "    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['decoder_h2']),\n",
    "                                   biases['decoder_b2']))\n",
    "    return layer_2\n",
    "\n",
    "def discriminator(x):\n",
    "    D_h1 = tf.nn.relu(tf.matmul(x, D_W1) + D_b1)\n",
    "    out = tf.matmul(D_h1, D_W2) + D_b2\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#G_sample = generator(z)\n",
    "encoder_op = encoder(X)\n",
    "decoder_op = decoder(encoder_op)\n",
    "\n",
    "# Prediction\n",
    "G_sample = decoder_op\n",
    "\n",
    "D_real = discriminator(X)\n",
    "D_fake = discriminator(G_sample)\n",
    "\n",
    "D_loss = tf.reduce_mean(D_real) - tf.reduce_mean(D_fake)\n",
    "G_loss = -tf.reduce_mean(D_fake)\n",
    "\n",
    "D_solver = (tf.train.RMSPropOptimizer(learning_rate=1e-4)\n",
    "            .minimize(-D_loss, var_list=theta_D))\n",
    "G_solver = (tf.train.RMSPropOptimizer(learning_rate=1e-4)\n",
    "            .minimize(G_loss))\n",
    "\n",
    "clip_D = [p.assign(tf.clip_by_value(p, -0.01, 0.01)) for p in theta_D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0; D loss: -0.02217; G_loss: -0.02238\n",
      "Iter: 100; D loss: 2.23; G_loss: 1.641\n",
      "Iter: 200; D loss: 2.169; G_loss: 1.495\n",
      "Iter: 300; D loss: 2.003; G_loss: 1.326\n",
      "Iter: 400; D loss: 1.784; G_loss: 1.134\n",
      "Iter: 500; D loss: 1.616; G_loss: 1.038\n",
      "Iter: 600; D loss: 1.546; G_loss: 0.8926\n",
      "Iter: 700; D loss: 1.362; G_loss: 0.8838\n",
      "Iter: 800; D loss: 1.335; G_loss: 0.8201\n",
      "Iter: 900; D loss: 1.183; G_loss: 0.7599\n",
      "Iter: 1000; D loss: 1.123; G_loss: 0.7445\n",
      "Iter: 1100; D loss: 1.073; G_loss: 0.7297\n",
      "Iter: 1200; D loss: 0.9821; G_loss: 0.7359\n",
      "Iter: 1300; D loss: 0.9642; G_loss: 0.7266\n",
      "Iter: 1400; D loss: 0.908; G_loss: 0.7482\n",
      "Iter: 1500; D loss: 0.8323; G_loss: 0.7344\n",
      "Iter: 1600; D loss: 0.7895; G_loss: 0.7203\n",
      "Iter: 1700; D loss: 0.7633; G_loss: 0.6893\n",
      "Iter: 1800; D loss: 0.7376; G_loss: 0.6439\n",
      "Iter: 1900; D loss: 0.7311; G_loss: 0.6418\n",
      "Iter: 2000; D loss: 0.7044; G_loss: 0.6227\n",
      "Iter: 2100; D loss: 0.6873; G_loss: 0.6123\n",
      "Iter: 2200; D loss: 0.6802; G_loss: 0.6055\n",
      "Iter: 2300; D loss: 0.6942; G_loss: 0.5623\n",
      "Iter: 2400; D loss: 0.6724; G_loss: 0.5652\n",
      "Iter: 2500; D loss: 0.6528; G_loss: 0.5254\n",
      "Iter: 2600; D loss: 0.6372; G_loss: 0.5361\n",
      "Iter: 2700; D loss: 0.6271; G_loss: 0.5017\n",
      "Iter: 2800; D loss: 0.6079; G_loss: 0.4787\n",
      "Iter: 2900; D loss: 0.5887; G_loss: 0.4389\n",
      "Iter: 3000; D loss: 0.5788; G_loss: 0.3984\n",
      "Iter: 3100; D loss: 0.5269; G_loss: 0.4126\n",
      "Iter: 3200; D loss: 0.5738; G_loss: 0.4609\n",
      "Iter: 3300; D loss: 0.5734; G_loss: 0.3977\n",
      "Iter: 3400; D loss: 0.5216; G_loss: 0.3728\n",
      "Iter: 3500; D loss: 0.5087; G_loss: 0.4054\n",
      "Iter: 3600; D loss: 0.4721; G_loss: 0.2796\n",
      "Iter: 3700; D loss: 0.4816; G_loss: 0.3729\n",
      "Iter: 3800; D loss: 0.4795; G_loss: 0.331\n",
      "Iter: 3900; D loss: 0.4804; G_loss: 0.2844\n",
      "Iter: 4000; D loss: 0.4707; G_loss: 0.3293\n",
      "Iter: 4100; D loss: 0.4617; G_loss: 0.3431\n",
      "Iter: 4200; D loss: 0.4621; G_loss: 0.3749\n",
      "Iter: 4300; D loss: 0.4644; G_loss: 0.3242\n",
      "Iter: 4400; D loss: 0.4418; G_loss: 0.2842\n",
      "Iter: 4500; D loss: 0.4354; G_loss: 0.2515\n",
      "Iter: 4600; D loss: 0.4628; G_loss: 0.2153\n",
      "Iter: 4700; D loss: 0.4354; G_loss: 0.35\n",
      "Iter: 4800; D loss: 0.419; G_loss: 0.2815\n",
      "Iter: 4900; D loss: 0.4389; G_loss: 0.2394\n",
      "Iter: 5000; D loss: 0.426; G_loss: 0.2557\n",
      "Iter: 5100; D loss: 0.4314; G_loss: 0.2588\n",
      "Iter: 5200; D loss: 0.4093; G_loss: 0.2844\n",
      "Iter: 5300; D loss: 0.4001; G_loss: 0.2798\n",
      "Iter: 5400; D loss: 0.3901; G_loss: 0.2229\n",
      "Iter: 5500; D loss: 0.3916; G_loss: 0.1988\n",
      "Iter: 5600; D loss: 0.3935; G_loss: 0.2892\n",
      "Iter: 5700; D loss: 0.3869; G_loss: 0.221\n",
      "Iter: 5800; D loss: 0.3919; G_loss: 0.1911\n",
      "Iter: 5900; D loss: 0.3895; G_loss: 0.196\n",
      "Iter: 6000; D loss: 0.4099; G_loss: 0.3202\n",
      "Iter: 6100; D loss: 0.3969; G_loss: 0.28\n",
      "Iter: 6200; D loss: 0.386; G_loss: 0.2269\n",
      "Iter: 6300; D loss: 0.3967; G_loss: 0.2879\n",
      "Iter: 6400; D loss: 0.3684; G_loss: 0.1297\n",
      "Iter: 6500; D loss: 0.4118; G_loss: 0.3582\n",
      "Iter: 6600; D loss: 0.388; G_loss: 0.2072\n",
      "Iter: 6700; D loss: 0.3867; G_loss: 0.1969\n",
      "Iter: 6800; D loss: 0.4077; G_loss: 0.2559\n",
      "Iter: 6900; D loss: 0.4106; G_loss: 0.1515\n",
      "Iter: 7000; D loss: 0.4066; G_loss: 0.1711\n",
      "Iter: 7100; D loss: 0.3829; G_loss: 0.2022\n",
      "Iter: 7200; D loss: 0.3798; G_loss: 0.1971\n",
      "Iter: 7300; D loss: 0.4053; G_loss: 0.1436\n",
      "Iter: 7400; D loss: 0.3871; G_loss: 0.2253\n",
      "Iter: 7500; D loss: 0.3816; G_loss: 0.3185\n",
      "Iter: 7600; D loss: 0.3949; G_loss: 0.3215\n",
      "Iter: 7700; D loss: 0.3897; G_loss: 0.2573\n",
      "Iter: 7800; D loss: 0.3833; G_loss: 0.1973\n",
      "Iter: 7900; D loss: 0.4023; G_loss: 0.2168\n",
      "Iter: 8000; D loss: 0.3927; G_loss: 0.2728\n",
      "Iter: 8100; D loss: 0.3619; G_loss: 0.1962\n",
      "Iter: 8200; D loss: 0.4155; G_loss: 0.1615\n",
      "Iter: 8300; D loss: 0.4125; G_loss: 0.3858\n",
      "Iter: 8400; D loss: 0.3138; G_loss: 0.01894\n",
      "Iter: 8500; D loss: 0.3712; G_loss: 0.2274\n",
      "Iter: 8600; D loss: 0.3962; G_loss: 0.2006\n",
      "Iter: 8700; D loss: 0.3248; G_loss: 0.09871\n",
      "Iter: 8800; D loss: 0.3769; G_loss: 0.1358\n",
      "Iter: 8900; D loss: 0.3463; G_loss: 0.1765\n",
      "Iter: 9000; D loss: 0.3749; G_loss: 0.2282\n",
      "Iter: 9100; D loss: 0.3705; G_loss: 0.1896\n",
      "Iter: 9200; D loss: 0.3786; G_loss: 0.2332\n",
      "Iter: 9300; D loss: 0.3632; G_loss: 0.2412\n",
      "Iter: 9400; D loss: 0.3738; G_loss: 0.1936\n",
      "Iter: 9500; D loss: 0.3856; G_loss: 0.1899\n",
      "Iter: 9600; D loss: 0.3614; G_loss: 0.1508\n",
      "Iter: 9700; D loss: 0.3491; G_loss: 0.08333\n",
      "Iter: 9800; D loss: 0.3924; G_loss: 0.1935\n",
      "Iter: 9900; D loss: 0.3495; G_loss: 0.1468\n",
      "Iter: 10000; D loss: 0.374; G_loss: 0.2047\n",
      "Iter: 10100; D loss: 0.3881; G_loss: 0.2118\n",
      "Iter: 10200; D loss: 0.3729; G_loss: 0.2145\n",
      "Iter: 10300; D loss: 0.361; G_loss: 0.274\n",
      "Iter: 10400; D loss: 0.3731; G_loss: 0.2936\n",
      "Iter: 10500; D loss: 0.3882; G_loss: 0.3057\n",
      "Iter: 10600; D loss: 0.3462; G_loss: 0.2363\n",
      "Iter: 10700; D loss: 0.401; G_loss: 0.283\n",
      "Iter: 10800; D loss: 0.3702; G_loss: 0.2751\n",
      "Iter: 10900; D loss: 0.3663; G_loss: 0.1698\n",
      "Iter: 11000; D loss: 0.3776; G_loss: 0.1528\n",
      "Iter: 11100; D loss: 0.3542; G_loss: 0.1887\n",
      "Iter: 11200; D loss: 0.3868; G_loss: 0.2561\n",
      "Iter: 11300; D loss: 0.3649; G_loss: 0.2052\n",
      "Iter: 11400; D loss: 0.3708; G_loss: 0.206\n",
      "Iter: 11500; D loss: 0.3703; G_loss: 0.18\n",
      "Iter: 11600; D loss: 0.3897; G_loss: 0.1512\n",
      "Iter: 11700; D loss: 0.367; G_loss: 0.2176\n",
      "Iter: 11800; D loss: 0.3812; G_loss: 0.1836\n",
      "Iter: 11900; D loss: 0.3605; G_loss: 0.1486\n",
      "Iter: 12000; D loss: 0.3656; G_loss: 0.2096\n",
      "Iter: 12100; D loss: 0.3959; G_loss: 0.3388\n",
      "Iter: 12200; D loss: 0.3688; G_loss: 0.1895\n",
      "Iter: 12300; D loss: 0.3786; G_loss: 0.2672\n",
      "Iter: 12400; D loss: 0.36; G_loss: 0.1991\n",
      "Iter: 12500; D loss: 0.3748; G_loss: 0.1819\n",
      "Iter: 12600; D loss: 0.3738; G_loss: 0.2589\n",
      "Iter: 12700; D loss: 0.3244; G_loss: 0.05421\n",
      "Iter: 12800; D loss: 0.3594; G_loss: 0.2357\n",
      "Iter: 12900; D loss: 0.3765; G_loss: 0.2024\n",
      "Iter: 13000; D loss: 0.3711; G_loss: 0.1654\n",
      "Iter: 13100; D loss: 0.3522; G_loss: 0.1358\n",
      "Iter: 13200; D loss: 0.3751; G_loss: 0.1056\n",
      "Iter: 13300; D loss: 0.3671; G_loss: 0.2166\n",
      "Iter: 13400; D loss: 0.3635; G_loss: 0.217\n",
      "Iter: 13500; D loss: 0.3717; G_loss: 0.2362\n",
      "Iter: 13600; D loss: 0.3577; G_loss: 0.2325\n",
      "Iter: 13700; D loss: 0.3521; G_loss: 0.09307\n",
      "Iter: 13800; D loss: 0.3789; G_loss: 0.1958\n",
      "Iter: 13900; D loss: 0.3647; G_loss: 0.2815\n",
      "Iter: 14000; D loss: 0.3904; G_loss: 0.3196\n",
      "Iter: 14100; D loss: 0.3934; G_loss: 0.3417\n",
      "Iter: 14200; D loss: 0.3532; G_loss: 0.1904\n",
      "Iter: 14300; D loss: 0.4019; G_loss: 0.1075\n",
      "Iter: 14400; D loss: 0.3461; G_loss: 0.08181\n",
      "Iter: 14500; D loss: 0.3254; G_loss: 0.04425\n",
      "Iter: 14600; D loss: 0.3888; G_loss: 0.1556\n",
      "Iter: 14700; D loss: 0.3735; G_loss: 0.2403\n",
      "Iter: 14800; D loss: 0.3406; G_loss: 0.1602\n",
      "Iter: 14900; D loss: 0.3654; G_loss: 0.2137\n",
      "Iter: 15000; D loss: 0.3508; G_loss: 0.1622\n",
      "Iter: 15100; D loss: 0.358; G_loss: 0.2683\n",
      "Iter: 15200; D loss: 0.3771; G_loss: 0.2172\n",
      "Iter: 15300; D loss: 0.3705; G_loss: 0.1751\n",
      "Iter: 15400; D loss: 0.3496; G_loss: 0.1849\n",
      "Iter: 15500; D loss: 0.3582; G_loss: 0.08782\n",
      "Iter: 15600; D loss: 0.3633; G_loss: 0.1322\n",
      "Iter: 15700; D loss: 0.3502; G_loss: 0.1722\n",
      "Iter: 15800; D loss: 0.3809; G_loss: 0.2827\n",
      "Iter: 15900; D loss: 0.3346; G_loss: 0.2258\n",
      "Iter: 16000; D loss: 0.3752; G_loss: 0.2802\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "if not os.path.exists('out/'):\n",
    "    os.makedirs('out/')\n",
    "\n",
    "i = 0\n",
    "\n",
    "for it in range(1000000):\n",
    "    for _ in range(5):\n",
    "        X_mb, _ = mnist.train.next_batch(mb_size)\n",
    "\n",
    "        _, D_loss_curr, _ = sess.run(\n",
    "            [D_solver, D_loss, clip_D],\n",
    "            feed_dict={X: X_mb}\n",
    "        )\n",
    "\n",
    "    _, G_loss_curr = sess.run(\n",
    "        [G_solver, G_loss],\n",
    "        feed_dict={X: X_mb}\n",
    "    )\n",
    "\n",
    "    if it % 100 == 0:\n",
    "        print('Iter: {}; D loss: {:.4}; G_loss: {:.4}'\n",
    "              .format(it, D_loss_curr, G_loss_curr))\n",
    "\n",
    "        if it % 1000 == 0:\n",
    "            samples = sess.run(G_sample, feed_dict={X: X_mb})\n",
    "            fig = plot( np.append(X_mb, samples, axis=0))\n",
    "            plt.savefig('out/{}.png'\n",
    "                        .format(str(i).zfill(3)), bbox_inches='tight')\n",
    "            i += 1\n",
    "            plt.close(fig)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
